#Author TheStair

#Imports
from binaryninja import *
import sys
from pathlib import Path
import os
import capa.rules
import capa.main
import capa.engine
import capa.features
import capa.loader
import re
from django.conf import settings

# Writes string of function names and outputs it to a file
# def list_file_function_names():

#     functions = bv.functions
#     str_of_funcs = ""
#     for func in functions:
#         str_of_funcs += f"Function {func.name} starts at 0x{func.start:x}\n"
    
#     output_file_path = os.path.join(output_folder, "list_of_functions.txt")
#     with open(output_file_path, "w") as out_file:
#         out_file.write(str_of_funcs)
#     print("List of Functions located in plugin_output/")


elf_signature = b'\x7F\x45\x4c\x46'
pe_signature = b'\x4d\x5a'

def is_elf(file_path):
    with open(file_path, 'rb') as f:
        header = f.read(4)
        return header == elf_signature
    
def is_pe(file_path):
    with open(file_path, 'rb') as f:
        header = f.read(2)
        return header == pe_signature
    



# Prompt for user input and return the filepath
input_file_path_str = ""
def get_input_file():
    global input_file_path_str
    input_file_path_str = input("Please Enter the File to be analyzed: ").strip().lower()
    if input_file_path_str != "":
        file_path = Path(input_file_path_str)
    else:
        print("Please Enter a Filepath")
        sys.exit()
    return file_path


# Capa seems to have very little public documentation.
# I used oxide to try and figure out how to get the information I want
# https://github.com/Program-Understanding/oxide/blob/main/src/oxide/modules/extractors/capa_results/module_interface.py

# Runs CAPA Analysis and returns a JSON for each function with a match.
# Each JSON contains the function name, address, matched rules, and disassembly
# After looking throught the CAPA repo, I found that capa has a binary ninja backend. So to avoid wrestling
# with different address spaces, I opted to use their binja backend
capabilities = []
overall_matches = []
def capa_analyze(file_path):
    global capabilities
    global overall_matches
    print("Running CAPA Analysis")
    # rules_path = Path("/home/thestair/Documents/Spring-2025/Program Analysis/CAPA-Match-Binja/capa-rules-9.1.0/")
    if is_elf(file_path):
        rules_path = os.path.join(settings.BASE_DIR, 'capa-rules-elf')
        rules_path = Path(rules_path)
    elif is_pe(file_path):
        rules_path = os.path.join(settings.BASE_DIR, 'capa-rules-pe')
        rules_path = Path(rules_path)
    else:
        print("Unsupported file type. Only ELF and PE files are supported.")
        return
    
    #Sets capa's path to the rules
    rules = capa.rules.get_rules([rules_path])

    #Sets the extractor CAPA will use 
    extractor = capa.loader.get_extractor(file_path, "auto", "auto", capa.main.BACKEND_BINJA, [], False, disable_progress=True,)
    
    #Runs the actual Analysis
    capabilities = capa.main.find_capabilities(
            rules, extractor, disable_progress=True
        )

    for rule_name, match_list in capabilities.matches.items():
        overall_matches.append(rule_name)
    # Fetches additional data (Not really used)
    # meta = capa.loader.collect_metadata(
    #         [], file_path, "auto", "auto", [rules_path], extractor, counts)



def load_binja(input_file):
    with load(input_file) as bv:
        #print("Loaded file into Binary Ninja")
        bv.update_analysis_and_wait()
    return bv

def parse_binja_headers(bv) -> dict:
    # BV will know its own format and entry-point
    return {
        "format":       bv.view_type,                   # "ELF" or "PE"
        "entry_point":  hex(bv.entry_point),
        "sections":     list(bv.sections.keys()),      # names â†’ Section objects
        "arch":         str(bv.arch),                  # architecture name
        "is_relocatable": bv.relocatable,              # True/False
    }


output_dir = ""
def generate_jsons(bv, input_file, headers):
    global output_dir
    global capabilities
    function_data = {}
    for rule_name, match_list in capabilities.matches.items():
        for match in match_list:
            feature_str = str(match[0])
            m = re.search(r'0x[0-9a-fA-F]+', feature_str)
            if not m:
                continue  # Skip if no address
            addr_str = m.group(0)
            addr_int = int(addr_str, 16)

            # Use Binary Ninja to find the function containing this address.
            funcs = bv.get_functions_containing(addr_int)
            if not funcs:
                continue 
            
            # For simplicity, choose the first function if there are several.
            func = funcs[0]
            func_name = func.name

            # If the function has not been seen, add it's disassembly
            if func_name not in function_data:
                disasm_lines = []
                decomp_lines = []
                for block in func.basic_blocks:
                    for line in block.disassembly_text:
                        disasm_lines.append(str(line))

                try:
                    hlil = func.hlil  # Accessing this triggers HLIL generation
                    for block in hlil.basic_blocks:
                        for instr in block:
                            decomp_lines.append(str(instr))
                except Exception as e:
                    decomp_lines.append("No High Level IL Loaded")
                            

                function_data[func_name] = {
                    "address": hex(func.start),
                    "rules": set(),  # We'll use a set to avoid duplicates.
                    "disassembly": disasm_lines,
                    "high_level_IL": decomp_lines,
                }

            # Add the rule name to the function's rule set.
            function_data[func_name]["rules"].add(rule_name)

    #Convert the rule matches for each function to a list
    for func in function_data:
        function_data[func]["rules"] = list(function_data[func]["rules"])
    
    base = os.path.basename(input_file)
    # Create the output directory name by appending "_matches"
    output_dir = os.path.join(settings.BASE_DIR,'analyzed_binaries', base, base + "_results")
    os.makedirs(output_dir, exist_ok=True)

    function_dir = os.path.join(output_dir, "functions")
    os.makedirs(function_dir, exist_ok=True)

    for func_name, data in function_data.items():
        output_file = os.path.join(function_dir, f"{func_name}.json")
        with open(output_file, "w") as out_file:
            json.dump(data, out_file, indent=2)

    output_file = os.path.join(output_dir, f"header_info.json")
    with open(output_file, "w") as out_file:
        json.dump(headers, out_file, indent=2)

    output_file = os.path.join(output_dir, f"overall_matches.json")
    with open(output_file, "w") as out_file:
        json.dump(overall_matches, out_file, indent=2)



    # print(f"Matched functions stored in {output_dir}/")

def full_analysis(input_file):
    global output_dir
    input_file = Path(input_file)
    # Load the file into Binary Ninja
    bv = load_binja(input_file)


    headers = parse_binja_headers(bv)

    capa_analyze(input_file)
    return generate_jsons(bv, input_file, headers)

    # Generate JSON files
    generate_jsons(bv, input_file)

    return output_dir

# ------------------------------------------------------------------------------------------------------------
# Script Start
# ------------------------------------------------------------------------------------------------------------
#Loads user-specified file
# t = True
# while t == True:
#     input_file = ""
#     input_file = get_input_file()


#     # Make output folder
#     # output_folder = "debugging_artifacts"
#     # os.makedirs(output_folder, exist_ok=True)

#     # Run analysis
#     capa_analyze(input_file)
#     generate_jsons(load_binja(input_file), input_file)
#     loop_check = input("Would you like to analyze another file? (y/n) ").strip().lower()

#     if loop_check == "y":
#         t = True
    
#     else:
#         print("\nGoodbye!")
#         sys.exit()
